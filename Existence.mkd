## Definition of trajectories, orbit, and linear stability  

(based on @Wiggins03aa, section 7)

Consider the following dynamical system:

\begin{equation}\label{eq:diff}
\dot x \eqdef \ddt x = f(x,t),\text{ with }x \in \Omega
\end{equation}

\bd{Autonomous dynamical system}
The system is said to be _autonomous_ if there is no explicit dependence of $f$ on $t$, and non-autonomous otherwise. 
\ed

For simplicity, we will admit that $\Omega = \mathbb{R}^n$, where $n$ is the phase-space dimension. 

@. A _trajectory_ $x(t, t_0, x_0)$ (or _phase curve_), for $t=I$ is a solution of the differential equation \eqref{eq:diff}, passing through $x_0$ at time $t_0$ over an interval of existence $I$. 
@. An orbit through $x_0$ is the set of points in phase space passing of a trajectory passing through $x_0$. 


## Equilibrium solutions, stability and linearised stability

In this section we briefly formalise and extend notions that have already been covered in the _Motivating example_ section. 

We consider an _autonomous_ vector field

\begin{displaymath}
\ddt x = f(x), \quad x \in \mathbb{R}^n
\end{displaymath}

A fixed point (also said: _equilibrium solution_) is a point $x_f \in \mathbb{R}^n$ such that $f(x_f)=0$. 

We distinguish the Lyapunov stability from the asymptotic stability. Roughly speaking, $x_f$ is _stable_ (shorthand for _Lyapunov stable_) if solutions starting close enough to $x_f$ remain near it. It is _asymptotically stable_ if these solutions eventually converge to $x_f$. 

\bd{Lyapunov Stability} The fixed point $x_f$ is (Lyapunov) stable if, given $\varepsilon > 0$, there exists a $\delta > 0$ such that, for any other trajectory  $y(t)$ satisfying $|x_f-y(t_0)| < \delta$, then $|x_f-y(t_0)| < \varepsilon$ for $t>t_0$, $t_0 \in \mathbb{R}$. 
\ed

\bd{Asmyptotic  Stability} A fixed point $x_f$ is asymptotically stable if it is Lyapunov stable _and_ for any other solution $y(t)$ there exists a constant $b>0$ such that, if $|x_f-y(t_0)| < 0$, then  $\lim_{t \rightarrow \infty} |x_f-y(t)| = 0$. 
\ed

\bcd
Illustrate these notions graphically
\ecd 

We now reconsider the notion of linear stability, based on the developments already started in the Motivation section. 

Consider a fixed point $x_f$, and let $y = x_f + \delta x$. 

We find that

\begin{displaymath}
\ddt {\delta x}_j = \sum \frac{\partial f_j}{\partial x_i}  \delta x_i  + \mathcal{O}(|\delta x| ^2),
\end{displaymath}

where the derivatives are evaluated in $x_f$, 
which we more simply write 
$$\ddt {\delta x}_j = \Dif f(x_f) \delta x + \mathcal{O}(|\delta x|^2).$$

The notation $\Dif f(x_f)$  refers to the Jacobian of $f$ evaluated in $x_f$. If $n=1$ (one-dimensional system), then $\Dif f$ is simply $\od{f}{x}$, which corresponds to the example in the Motivation section. 

Given that for the stability of fixed points we are only concerned with the behaviour of solutions arbitrarily close to $x_f$, it seems reasonable to inspect the linear system: 

\begin{displaymath}
\ddt {\delta x}_j = \Dif f(x_f) \delta x,
\end{displaymath}

...of which we know the solution! We know that it will grow exponentially in the direction of eigenvectors of the Jacobian with positive eigenvalues, and decay exponentially in the direction of eigenvectors with negative eigenvalues. 

\bd{Hyperbolic fixed point.} A fixed point is hyperbolic if all the eigenvalues of $\Dif f(x_f)$ have either a strictly positive or a strictly negative real part.
\ed

We will be able to _prove_ that a fixed point with all _negative_ eigenvalues (said to be linearly stable) is asymptotically stable. But we leave it for later. At this point, we already know enough to understand the idea of sink and source:

1. A hyperbolic fixed point is called a _sink_ if all eigenvalues of the Lyapunov spectrum have negative real parts. 
2. A hyperbolic fixed point is called a _source_ if all eigenvalues of the Lyapunov spectrum have positive real parts.
3. A hyperbolic fixed point is called a _saddle_ if some but not all eigenvalues of the Lyapunov spectrum have positive real parts
4. A non-hyperbolic fixed point with purely imaginary eigenvalues, and non-zero is a _center_.


\bcd
Consider again the motivating example with the ice sheets. Are the fixed points hyperbolic? Everywhere ? (tip: it is enough  to restrict the discussion to the case $x \neq 0$. )
\ecd 

## Properties of vector fields: existence, uniqueness, differentiability and flows


In this section we develop a technical apparatus to deal wit the notions of "long term" and "observable" behaviours of _orbits_ of _dynamical systems_. This will later allow us to develop the notions of _attracting sets_ and _attractors_. Again, we  restrict the discussion to dynamical systems in the form of vector fields, that is: 

\begin{equation}\label{eq:diff2}
\ddt x = f(x,t),\text{ with }x \in \Omega
\end{equation}

and, again, for simplicity, we will admit that $\Omega = \mathbb{R}^n$. Here, we further suppose that $f(x,t)$ is $\mathbf{C}^r$-differentiable, with $r \geq 1$ on the open set $U \subset \mathbb{R}^n\times \mathbb{R}$. 

\bt{Existence} Let $(x_0,t_0)\in U$. Then, there _exists_ a solution of \eqref{eq:diff2} through the point $x_0$ at $t=t_0$, denoted $x(t,t_0,x_0)$ for $|t-t_0|$ sufficiently small. The solution is unique. Moreover, $x(t,t_0,x_0)$ is a $\mathbb{C}^r$ function of $t$, $t_0$, and $x_0$. 
\et

The proof is available in specialised books (e.g. Hirsch and Smale, 1974). 

It has also been proved that the  unique solution can be (uniquely)  _extended_ to the boundaries of any closed, compact subset of $U$. However, this says nothing about what is going on once the solution has reached the boundaries ! 

\bcd
Consider the following example: $\ddt x = x^2,\quad x\in \mathbb{R}$. Find the analytical solution, and show that the solution blows up and that the interval of existence of solutions through $x_0$ at $t=0$ depends on $x_0$. Tip: use the methods of variable separation. 
\ecd 

There in another handy theorem, that says that if $f(x,t,\psi)$ is differentiable with respect to the parameter $\psi$, then the solution is also differentiable with respect to that parameter. 

## Special properties of autonomous fields 

We now consider two important propositions that apply to autonomous dynamical systems, and that we will (at last!) be able to prove. For simplicity, we consider a dynamical system admitting solutions over all times.

\bt{Time shifted trajectories are trajectories in autonomous fields.} 
If $x(t)$ is a solution of $\ddt x = f(x)$, then so is $x(t+\tau)$ for any $\tau$. 
\et 

Proof: By definition, $\ddt {x(t)} = f\left( x \left(t\right) \right).$ Hence, we have

\begin{displaymath}
\left. \ddt {x(t+\tau)}\right|_{t=t_0} = 
\left. \ddt {x(t)}\right|_{t=t_0+\tau} = 
f\left( x \left (t + \tau \right) \right) =  
\left. f\left( x \left (t_0 + \tau \right) \right) \right|_{t=t_0}.  
\end{displaymath}

To pack this up: 

$\left. \ddt {x(t+\tau)}\right|_{t=t_0} = 
\left. f\left( x \left (t + \tau \right) \right) \right|_{t=t_0}$ is true for any $t_0\in\mathbb{R}$. In other words, $x(t+\tau)$ is a solution of the dynamical system. \hfill $\square$ 

The formal maths make it a bit confusing, but the intuition is reasonably  clear: two time-shifted trajectories that pass through the same point _correspond to the same orbit_ ! 

This has an important implication: in an autonomous dynamical system, there is _one single_ orbit that passes through any point $x_0$. There is a more formal proof in @Wiggins03aa, but the intuition seems to be reasonably clear from the uniqueness theorem.

In other words, 

\bt{Non-crossing orbits} Orbits _never cross_ each other in an autonomous dynamical system. 
\et 

\bcd
Consider the following flow diagram. How would you prove that the intersection is necessarily a fixed point? What does it say about the time a trajectory will need to reach the fixed point? 

\includegraphics[page=4, angle=90, scale=0.7, trim={0 0 0 3cm}, clip]{lphys2114-figures}

\ecd 

## Flows

At this point, we have understood (at least intuitively) that the future fate a point in the phase space, in an autonomous system, does not depend on the time at which the snapshot has been taken. This is after all quite straightforward given the definition of an autonomous system (the evolution dictated by $f$ does not depend explicitly on time). In other words, for any point $(x_0,t_0)$, there will be a function $\phi^\tau(x_0)$ that points to the point of the orbit reached at time $t+\tau$,  as long as the trajectory has remained within the bounds of the domain. This function $\phi$ is the _flow function_, sometimes called the _phase flow_ function. 

Given the uniqueness of solutions, we find that $\phi^{\tau_1+\tau_2} = \phi^{\tau_2} \circ \phi^{\tau_2}$ and $\phi^0 = \phi^{\tau_1-\tau_1} = Id.$. 

From where it comes that the flow is an invertible function:

$$\left(\phi^{\tau}\right )^{-1} = \phi^{-\tau}.$$

In our notation we have been a little ambiguous about the domain of $\phi$, something programmers will not like (one should always specify the type of inputs of a function). Above we have applied $\phi^\tau$ to elements of the phase-space domain. However, we could also map the function on subsets $U$ of this domain, as illustrated on the figure below:

$$\phi^{\tau_1+\tau_2}(U) = \phi^{\tau_2} \circ \phi^{\tau_1} (U).$$ Again, this flow function is invertible.

\includegraphics[page=5, angle=90, scale=0.7]{lphys2114-figures}

So the flow function provides an intuitive impression that is reminiscent of a fluid flow. It may be converging; diverging but streamlines never cross each other. At this point, we may already perceive that dynamical systems will fall in different categories. 

1. Those associated with flow functions that (on average) diverge. Trajectories tend to spread over. Such flows can be considered to be globally unstable because trajectories tend to grow towards the limits of the domain.
2. Those associated with flow functions that (on average) converge. Trajectories tend to cluster towards attracting regions (a notion that still needs to be formalised). These flows are called _dissipative_.
3. Those associated with flow functions that neither diverge nor converge. The phase-space volume of $U$ is conserved through time. 

\bcd
The latter category represents the class of _Hamiltonian flows_. Can you guess why?  Tip: think of Liouville's theorem. 
\ecd 

\bcd
At this point we may already have some intuitions about how the eigenvectors and eigenvalues of the Jacobian of $f$ governs the properties of the flow.  Which conditions do we expect for a converging flow? What happens if at least one eigenvalue is positive? The eigenvalues of the Jacobian are the _Lyapunov spectrum_ and the individual values are called the _Lyapunov exponents_. Why this term: "exponent". In non-linear dynamics we tend to pay attention to the _largest Lyapunov exponent_. Can you guess why ? 
\ecd

As we will make it clear in the following, _dissipative flows_ are characterised by a _negative Jacobian trace_: the sum of eigenvalues is negative; $\sum\lambda_i < 0$.  

The figure below represents a _dissipative_ flow that has at least one positive Lyapunov exponent. The phase space volume is shrinking over time (meaning that the _sum_ of eigenvalues is negative, even though one is positive). However, because one eigenvalue is positive, the flow gets stretched in one direction, so that to initially close initial conditions get increasingly distant with each other as time grows. 

\includegraphics[page=6, angle=90, scale=0.7]{lphys2114-figures}

This is the basic explanation to the possibility of sensitive dependence to initial conditions, that is, chaos. 

One generally distinguishes dissipative chaos (in dissipative flows) from Hamiltonian chaos (in Hamiltonian flows). Poincaré first discovered sensitive dependence to initial conditions in the 3-body problem, which is characterised by a Hamiltonian flow. The meteorologist Edwar Lorenz is famous for having popularised the notion of dissipative chaos.   We will come back to these notions later on. 


