## Definition of trajectories, orbit, and linear stability  

(based on @Wiggins03aa, section 7)

Consider the following dynamical system:

\begin{equation}\label{eq:diff}
\dot{\vec{x}} \eqdef \ddt \vec{x} = \vec{f}(\vec{x},t),\text{ with }\vec{x} \in \Omega
\end{equation}

\bd{Autonomous dynamical system}
The system is said to be _autonomous_\index{autonomous} if there is no explicit dependence of $\vec{f}$ on $t$, and non-autonomous otherwise. 
\ed

For simplicity, we will admit that $\Omega = \mathbb{R}^n$, where $n$ is the phase-space dimension. 

\bd{Trajectory}\label{def:trajectory}
A _trajectory_\index{trajectory} $\vec{x}(t, t_0, x_0)$ (or _phase curve_), for $t\in I$ is a solution of the differential equation \eqref{eq:diff}, passing through $\vec{x}_0$ at time $t_0$ over an interval of existence $I$. 
\ed

\bd{Orbit}
An orbit through $\vec{x}_0$ is the set of points in phase space passing of a trajectory passing through $\vec{x}_0$. 
\ed

## Equilibrium solutions, stability and asymptotic stability

In this section we briefly formalise and extend notions that have already been introduced in the motivating chapter (section \ref{sect:icesheet}). 

We consider an _autonomous_ vector field

\begin{displaymath}
\ddt{\vec{x}} = \vec{f}(\vec{x}), \quad \vec{x} \in \mathbb{R}^n
\end{displaymath}

A fixed point\index{fixed point} (also said: _equilibrium solution_) is a point $\vec{x}_f \in \mathbb{R}^n$ such that $f(\vec{x}_f)=0$. 

We distinguish the Lyapunov stability from the asymptotic stability. Roughly speaking, $\vec{x}_f$ is _stable_ (shorthand for _Lyapunov stable_) if solutions starting close enough to $\vec{x}_f$ remain near it. It is _asymptotically stable_ if these solutions eventually converge to $\vec{x}_f$. 

\bd{Lyapunov Stability}\index{Lyapunov stability} The fixed point $\vec{x}_f$ is (Lyapunov) stable if, given $\varepsilon > 0$, there exists a $\delta > 0$ such that, for any other trajectory  $y(t)$ satisfying $|\vec{x}_f-\vec{y}(t_0)| < \delta$, then $|\vec{x}_f-\vec{y}(t)| < \varepsilon$ for $t>t_0$, $t_0 \in \mathbb{R}$. 
\ed

\bd{Asymptotic  Stability}\index{asymptotic stability} A fixed point $x_f$ is asymptotically stable if it is Lyapunov stable _and_ for any other solution $y(t)$ there exists a constant $b>0$ such that, if $|\vec{x}_f-\vec{y}(t_0)| < b$, then  $\lim_{t \rightarrow \infty} |\vec{x}_f-\vec{y}(t)| = 0$. 
\ed

\bcd
Illustrate these notions graphically
\ecd 

<!-- 
We now reconsider the notion of linear stability, based on the developments already started in the Motivation section. 

Consider a fixed point $x_f$, and let $y = x_f + \delta x$. 

We find that

\begin{displaymath}
\left(\ddt {\delta x}\right)_j = \sum \frac{\partial f_j}{\partial x_i}  \delta x_i  + \mathcal{O}(|\delta x| ^2),
\end{displaymath}

where the derivatives are evaluated in $x_f$, and $f_j$ is the $j^\text{th}$ component of $f$, 
which we more simply write 
$$\ddt {\delta x} = \Dif f(x_f) \delta x + \mathcal{O}(|\delta x|^2).$$

The notation $\Dif f(x_f)$  refers to the Jacobian of $f$ evaluated in $x_f$. If $n=1$ (one-dimensional system), then $\Dif f$ is simply $\od{f}{x}$, which corresponds to the example in the Motivation section. 

Given that for the stability of fixed points we are only concerned with the behaviour of solutions arbitrarily close to $x_f$, it seems reasonable to inspect the linear system: 

\begin{displaymath}
\ddt {\delta x} = \Dif f(x_f) \delta x,
\end{displaymath}

We will inspect more carefully the solutions of this system in the plane in section \ref{sect:plane} but we may already anticipate that  solutions will grow exponentially in the direction of eigenvectors of the Jacobian with positive eigenvalues, and decay exponentially in the direction of eigenvectors with negative eigenvalues. 

\bd{Hyperbolic fixed point.} A fixed point is hyperbolic if all the eigenvalues of $\Dif f(x_f)$ have either a strictly positive or a strictly negative real part.
\ed

We will be able to _prove_ that a fixed point with all _negative_ eigenvalues (said to be linearly stable) is asymptotically stable. But we leave it for later. At this point, we already know enough to understand the idea of sink and source:

1. A hyperbolic fixed point is called a _sink_ if all eigenvalues of the Lyapunov spectrum have negative real parts. 
2. A hyperbolic fixed point is called a _source_ if all eigenvalues of the Lyapunov spectrum have positive real parts.
3. A hyperbolic fixed point is called a _saddle_ if some but not all eigenvalues of the Lyapunov spectrum have positive real parts
4. A non-hyperbolic fixed point with purely imaginary eigenvalues, and non-zero is a _center_.


\bcd
Consider again the motivating example with the ice sheets. Are the fixed points hyperbolic? Everywhere ? (tip: it is enough  to restrict the discussion to the case $x \neq 0$. )
\ecd 

-->

## Properties of vector fields: existence, uniqueness, differentiability and flows


In this section we develop a technical notion to deal wit the notions of "long term" and "observable" behaviours of _orbits_\index{orbit} of _dynamical systems_. This will later allow us to develop the notions of _attracting sets_ and _attractors_. Again, we  restrict the discussion to dynamical systems in the form of vector fields, that is: 

\begin{equation}\label{eq:diff2}
\ddt \vec{x} = f(\vec{x},t),\text{ with }\vec{x} \in \Omega
\end{equation}

and, again, for simplicity, we will admit that $\Omega = \mathbb{R}^n$. Here, we further suppose that $\vec{f}(\vec{x},t)$ is $\mathbf{C}^r$-differentiable, with $r \geq 1$ on the open set $U \subset \mathbb{R}^n\times \mathbb{R}$. 

\bt{Existence} Let $(\vec{x}_0,t_0)\in U$. Then, there _exists_ a solution of \eqref{eq:diff2} through the point $\vec{x}_0$ at $t=t_0$, denoted $\vec{x}(t,t_0,\vec{x}_0)$ for $|t-t_0|$ sufficiently small. The solution is unique. Moreover, $\vec{x}(t,t_0,\vec{x}_0)$ is a $\mathbb{C}^r$ function of $t$, $t_0$, and $\vec{x}_0$. 
\et

The proof is available in specialised books \cite[e.g.][chap. 6 for linear systems and chap. 17 for more general systems]{hirsh13aa}.  

It has also been proved that the unique solution can be (uniquely)  _extended_ to the boundaries of any closed, compact subset of $U$. However, this says nothing about what is going on once the solution has reached the boundaries.  

\bcd
Consider the following example: $\ddt x = x^2,\quad x\in \mathbb{R}$. Find the analytical solution, and show that the solution blows up and that the interval of existence of solutions through $x_0$ at $t=0$ depends on $x_0$. Tip: use the methods of variable separation. 
\ecd 

There in another handy theorem, that says that if $\vec{f}(\vec{x},t,\psi)$ is differentiable with respect to the parameter $\psi$, then the solution is also differentiable with respect to that parameter. 

## Special properties of autonomous fields 

We now consider two important propositions that apply to autonomous dynamical systems, and that we will be able to prove. For simplicity, we consider a dynamical system admitting solutions over all times.

\bt{Time shifted trajectories are trajectories in autonomous fields.} 
If $\vec{x}(t)$ is a solution of $\ddt \vec{x} = \vec{f}(\vec{x})$, then $\vec{x}(t+\tau)$ is also a solution of that equation for any $\tau$. 
\et 

Proof: By definition, $\ddt {\vec{x}(t)} = \vec{f}\left( \vec{x} \left(t\right) \right).$ Hence, we have

\begin{displaymath}
\left. \ddt {\vec{x}(t+\tau)}\right|_{t=t_0} = 
\left. \ddt {\vec{x}(t)}\right|_{t=t_0+\tau} = 
\vec{f}\left( \vec{x} \left (t_0 + \tau \right) \right) =  
\left. f\left( \vec{x} \left (t + \tau \right) \right) \right|_{t=t_0}.  
\end{displaymath}

To pack this up: 

$\left. \ddt {\vec{x}(t+\tau)}\right|_{t=t_0} = 
\left. \vec{f}\left( \vec{x} \left (t + \tau \right) \right) \right|_{t=t_0}$ is true for any $t_0\in\mathbb{R}$. In other words, $\vec{x}(t+\tau)$ is a solution of the dynamical system. \hfill $\square$ 

The formal maths make it a bit confusing, but the intuition is reasonably  clear: two time-shifted trajectories that pass through the same point _correspond to the same orbit_ ! 

This has an important implication: in an autonomous dynamical system, there is _one single_ orbit that passes through any point $\vec{x}_0$. There is a more formal proof in @Wiggins03aa, but the intuition seems to be reasonably clear from the uniqueness theorem.

In other words, 

\bt{Non-crossing orbits} Orbits _never cross_ each other in an autonomous dynamical system. 
\et 

\bcd
Consider the following diagram. This is a phase portrait\index{phase portrait}: a diagram which qualitatively represents some trajectories such as to provide a general idea of the behavour of trajectories in the phase spaced. In this case, several trajectories seem to converge to a same point.  How would you prove that the intersection is necessarily a fixed point? What does it say about the time a trajectory will need to reach the fixed point? 

\includegraphics[page=4, angle=90, scale=0.7, trim={0 0 0 3cm}, clip]{lphys2114-figures}

\ecd 

## Flows

At this point, we have understood (at least intuitively) that the future fate a point in the phase space, in an autonomous system, does not depend on the time at which the snapshot has been taken. This is after all consistent  with the definition of an autonomous system (the evolution dictated by $\vec{f}$ does not depend explicitly on time). In other words, for any point $(\vec{x}_0,t_0)$, the point of the trajectory $\vec{x}(t,t_0,\vec{x_0})$ reached at time $t$ depends only on the initial condition $t_0$, and the time elapsed $\tau = t - t_0$. We can thus define a function $\phi(t-t_0, \vec{x_0})=\vec{x}(t,t_0,\vec{x_0})$. 


We proceed in two steps. First, we denote the family of functions $\phi^\tau$, defined such that $\phi^\tau(\vec{x_0})=x(t_0+\tau, t_0, \vec{x_0})$ which, we have seen, is independent of $t_0$. 
We observe the following properties: 

- $\phi^0$ is the identify function;
- $\left(\phi^{\tau}\right )^{-1} = \phi^{-\tau}:$ $\phi_\tau$ is thus an invertible function. 
- $\phi^t \in \mathbb{C}^r$.

\bd{Flow of an autonomous differential equation}\index{flow}
The family of functions $\phi\tau$ associated with the differential equation $\dot{\vec{x}} = \vec{f}(\vec{x})$, $\vec{f}\in\mathbb C^r$,  which satisfy the above properties, is called a flow. 
\ed

In our notation we have been a little ambiguous about the domain of $\phi$. Above we have applied $\phi^\tau$ to elements of the phase-space domain. However, we could also map the function on subsets $U$ of this domain, as illustrated on the figure below:

$$\phi^{\tau_1+\tau_2}(U) = \phi^{\tau_2} \circ \phi^{\tau_1} (U).$$ Again, this flow function is invertible.

<!--\includegraphics[page=5, angle=90, scale=0.7]{lphys2114-figures}-->
\input{Figures/flow.tikz}

\label{subsect:flow}

Given that we have required the vector field defined by $\vec{f}$ to be continuous and differentiable, the mapping defined by the flow $\phi$ also inherits some nice properties as we have seen it: it is invertible and differentiable. Therefore, it turns out that the flow generated by the vector field defined on $\mathbb{R}^n$ belongs to a wider class of mathematical objects called _diffeomorphism_\index{diffeomorphism}, which are continuous, invertible mappings of so-called "differential manifolds". The vector space defined on $\mathbb{R}^n$ is a particular case of differential manifold. This establishes a link between ordinary differential equations and differential geometry. The term "diffeomorphism" will therefore appear from time to time in some of the definitions below, specifically in section \label{sect:lyapunov} where we define attractors and attracting sets. 

The flow\index{flow} provides an intuitive impression that is reminiscent of a fluid flow. It may be converging; diverging but streamlines never cross each other. 


At this point, we may already perceive that dynamical systems will fall in different categories. A bit informally, we will mainly encounter and distinguish

1. Those associated with flow which (on average) diverge. Such flows can be considered to be globally unstable because trajectories tend to grow towards the limits of the domain; such dynamical systems have only local validity: they cannot be used to study asymptotic behaviours as time unfolds.
2. Those associated with flows that (on average) converge. Trajectories tend to cluster towards attracting regions (a notion that still needs to be formalised). These flows are called _dissipative_ \index{dissipative}: information about initial is, in practice, lost because a potentially large volume of initial conditions gets collapsed into a small volume of possible final states (the attractor);
3. Those associated with flow that neither diverge nor converge. The phase-space volume of $U$ is conserved through time. 

\bcd
The latter category represents the class of _Hamiltonian flows_\index{hamiltonian flow}. Can you guess why?  Tip: think of Liouville's theorem. 
\label{bcd:Ham1}
\ecd 

At this point we may already have some intuitions which will be generalised in the coming chapters. The stability or instability of fixed points in an autonomous system is related to the overall structure of the vector field defined by the function $\vec{f}$ (which is here a vector). If it is rather diverging around the fixed point we expect instability; if it is converging, stability.  In a 2-dimensional vector field, divergence and convergence can be assessed with the divergence operator $\nabla\cdot \vec{f}$. This is the trace of the Jacobian of the vector field, and it  measures the overall compression of the flow.  However, we may also anticipate that we might have divergence in one direction, and perhaps compression in another. These directions, as we will find out, are the eigenvectors of the Jacobian, and the relatad stability are given by the related eigenvalues. 



The figure below represents a _dissipative_ flow that has at least one positive Lyapunov exponent. The phase space volume is shrinking over time (meaning that the _sum_ of eigenvalues is negative, even though one is positive). However, because one eigenvalue is positive, the flow gets stretched in one direction, so that to initially close initial conditions get increasingly distant with each other as time grows. 

\includegraphics[page=6, angle=90, scale=0.7]{lphys2114-figures}

This is the basic explanation to the possibility of sensitive dependence to initial conditions, that is one of the cornerstones of chaos. 


One generally distinguishes dissipative chaos (in dissipative flows) from Hamiltonian chaos (in Hamiltonian flows). Poincaré first discovered sensitive dependence to initial conditions in the 3-body problem, which is characterised by a Hamiltonian flow. The meteorologist Edwar Lorenz is famous for having popularised the notion of dissipative chaos.

All these notions will have to be made more precise and proved, but hopefully the relevant ideas are already pretty much in place. 


